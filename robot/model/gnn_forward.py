# Baisic GNN forward engine
# The code containts some model similar to ``Graph Networks as Learnable Physics Engines for Inference and Control''

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.utils import scatter_
from torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool
from torch_geometric.data import Data

from robot.utils.models import fc

import numpy as np

def mlp(inp_dim, oup_dim, feature, num_layer, batch_norm):
    assert num_layer < 10
    if num_layer == 1:
        return fc(inp_dim, oup_dim)
    return nn.Sequential(*([fc(inp_dim, feature, relu=True, batch_norm=batch_norm)] +
                         [fc(feature, feature, relu=True, batch_norm=batch_norm) for i in range(num_layer-2)]+
                         [fc(feature, oup_dim, relu=False, batch_norm=False)]))


def batch_graphs(batch_size, n, graph):
    """
    :return: graph, batch
    """
    batch = torch.arange(batch_size)[:, None].expand(n).view(-1).contiguous()
    graph = graph[None,:] + (torch.arange(batch_size)[:, None] * n)
    graph = graph.permute(1, 0, 2).view(2, -1).contiguous()
    return graph, batch


class GNBlock(nn.Module):
    def __init__(self, node_channels, edge_channels, layers, mid_channels, global_channels=None,
                 output_node_channels=None, output_edge_channels=None, output_global_channels=None):
        super(GNBlock, self).__init__()
        if output_node_channels is None:
            output_node_channels = node_channels
        if output_edge_channels is None:
            output_edge_channels = mid_channels
        if output_global_channels is None:
            output_global_channels = global_channels

        self.edge_mlp = mlp(global_channels + node_channels * 2 + edge_channels,
                            output_edge_channels, mid_channels, layers, batch_norm=False)
        self.node_mlp = mlp(global_channels + node_channels + edge_channels,
                            output_node_channels, mid_channels, layers, batch_norm=False)


        if output_global_channels is not None:
            self.global_mlp = mlp(global_channels + output_node_channels + output_edge_channels,
                                  output_global_channels, mid_channels, layers, batch_norm=False)
        else:
            self.global_mlp = None

    def forward(self, node, edge, edge_index: torch.LongTensor, batch, g=None):
        rol, col = edge_index

        edge_inp = [node[rol], node[col], edge]
        if g is not None:
            edge_batch = batch[rol]
            edge_inp.append(g[edge_batch])
        edge_inp = torch.cat(edge_inp, dim=1)
        edge = self.edge_mlp(edge_inp)


        node_inp = [node, scatter_('add', rol, edge, dim=0)]
        if g is not None:
            node_inp.append(g[batch])
        node_inp = torch.cat(node_inp, dim=1)
        node = self.node_mlp(node_inp)

        if self.global_mlp is not None:
            global_inp = [scatter_('add', batch, node, dim=0), scatter_('edge', batch[rol], edge, dim=0)]
            if g is not None:
                global_inp.append(global_inp)
            g = self.global_mlp(torch.cat(global_inp), dim=1)
        return edge, node, g


class GNResidule(nn.Module):
    """
    Forward model for fixed model.
    The graph is always predefined.
    """
    def __init__(self, state_dim, action_dim, node_attr, edge_attr, graph, layers=3, mid_channels=256):
        super(GNResidule, self).__init__()
        self.node_attr = nn.Parameter(node_attr, requires_grad=False)

        # graph is a directed graph, generated by a dfs algorithm according to the robot arm's convention.
        # add direction as feature and repeat the edge
        edge_attr = torch.cat((edge_attr, edge_attr[:, -1:]*0+1), dim=1) # add sign
        edge_attr = torch.cat((edge_attr, edge_attr), dim=0)
        edge_attr[edge_attr.shape[0]//2, -1] *= -1
        self.edge_attr = nn.Parameter(edge_attr, requires_grad=False)

        # repeat all edge index
        graph = torch.cat((graph, graph), dim=1)
        self.graph = nn.Parameter(graph, requires_grad=False)

        self.state_dim = state_dim
        self.action_dim = action_dim

        node_dim = state_dim + self.node_attr.shape[1]
        edge_dim = action_dim + self.edge_attr.shape[1]


        self.gn1 = GNBlock(node_dim, edge_dim, layers, mid_channels,
                           output_node_channels=mid_channels,
                           output_edge_channels=mid_channels) # we don't use global now
        self.gn2 = GNBlock(node_dim + mid_channels, edge_dim + mid_channels, layers, mid_channels,
                           output_node_channels=state_dim)

    def build_graph(self, state, action):
        """
        :param state: (batch, n, d_x)
        :param action: (batch, e, action)
        """
        batch_size = state.shape[0]
        node = torch.cat((state, self.node_attr[None,:].expand(batch_size, -1, -1)), dim=2)
        edge = torch.cat((action, action), dim=1) # duplicate actions
        edge = torch.cat((edge, self.edge_attr[None,:].expand(batch_size, -1, -1)), dim=2)

        _, n, d = node.shape
        _, n_e, d_a = edge.shape

        node = node.view(-1, d)
        edge = edge.view(-1, d_a)
        self.n = n

        graph, batch = batch_graphs(batch_size, n, self.graph)
        return node, edge, graph, batch

    def decode(self, node, batch_size):
        return node.view(batch_size, self.n, -1)

    def forward(self, state, action, g=None):
        node, edge, graph, batch = self.build_graph(state, action)
        node, edge, g = self.gn1(node, edge, graph, batch, g)
        node, edge, g = self.gn2(node, edge, graph, batch, g)
        return self.decode(node, len(state))


class Wrapper:
    def __init__(self):
        pass
