# An improved environment
We need a new kind of environment, otherwise it would be very hard to write the code/handle the environments...

The goal of this project is to design and implement a suitable framework that can help me implement the idea easily.

The idea, is to separate the type of the observation, the observation, and the simulator.

- We have observation_space and action_space just like gym, which describe the type of the image.
- We should add the type OrderedDict/Graph/Node/List/Language(?) into the observation space. We should be able to serialize the type into numpy/pytorch array, and construct the type from the serialized type.
- For each observation type, we define the add/del/norm as metric space, and low, high as the range for continuous variable, set.
- Action is another kind of observation.
- We support the cost/goal/description/program  over the 
- For those information that would not change during the environment, we would store it into a variable called scene. If scene is not None, it will be a variable that would never change across one episode. Use reset to return that variable is not good. Use function global to get the env's global info. 

A framebuffer dataset, contains multiple episode information.

A very hard problem is to decide the output of observation, we think it should be the class before serialization, I mean, it should be a class. 

# space and observation
A space should support the following operator. Each space is a definition of certain type.

Type, could be either a numpy array/torch.Tensor, or a class that supports the following operators:
- from_numpy/from_torch: deserialize the sequence from pytorch/numpy array
- numpy/torch: translate to tensor
- One can transfer it to numpy/pytorch with \_\_array\_\_
- is_batch, A variable to decide if this is a batched one
- if it is_batched, we can use \_\_index\_\_ to extract some variable.

As for a space:
- from_numpy: transforms a numpy into the tensor (is_batched...)
- ~~to_jsonable~~
- ~~from_jsonable~~
- shape: hyper parameters for array/
- dtype: flaot32 or int64
- type: np.ndarray
- sample: inherited from gym, return the corresponding type, default, it will be a numpy-based tensor/List/Dict
- seed: inherited from gym
- contains: inherited from gym, which will be called by \_\_contains\_\_


Two keys:
1. We separate the implementation of scenen/observation for fast experiments.
2. We separate the implementation of restore and observation, as usually we can't really reconstruct the input by the scene.
3. We should also define the interface of a observation type to the network input... we call it encode: return a dict/list, which corresponding to the inputs of the network...



Considering the model-based RL training:
1. given action, we sampled scene and the serailization of the trajectory.
2. we sample a batch of things from the replay buffer.
3. from the batch, we construct the observation type.
4. use .observe(mode='default') to extract a subset of the variables [dict, list, tensor, np.ndarray] as the direct input to the neural network.
5. calculate the differences between the states or calculate the update of the states...and we then update the hiddens.




主要目的：服务于model based rl
1. 当scene与object是区分的时候，我们区分了global和observation。
2. 我们通常不能通过observation restore场景，需要call，global_raw与raw来获得这些信息。
3. 网络的输出和输入不一定是同一格式，但是一定要可以从输出格式变成输入格式，所以有observation space...以及output space?以及一个从output space往observation space的一个map？通常这两个是一样的。

考虑mujoco之类的，目标是：
hidden angle是可+的，observation中不包含global的position：这个属于global property。

现在的逻辑是：
scene is the fixed part..
encode is the part related to dX, usually it's identity
dX is the differential part. Encode is the part that relates to dX
f(project(x)) is used to predict dX, which is another space.. project will project the state to the one only related to dX

We use the differentiable environment, which has the following properties:

At the beginning the environment will reset the scene and variable.
- scene(default=None), the fixed part for the episode
- variable: description of the current variable or the hidden space. 

As the time goes by, the variable will change, and thus change the state of the whole world, thus we can recover the frame (time_step) variable.

frame is associated with each environment... that is the output of step and reset function should be a frame...
- frame (time_step): the variable that abstract all we know about a timestep
- frame.scene: get the fixed part of the frame... this is usually not very important
- frame.state: the variable that control the differences between frames
- frame.observe, from state, one can observe the state with observe
- frame.evaluate(goal): evaluate if one goal has been achieved. we design this for goal-conditioned RL
- frame.derivative(t): calculate the differences between two frames # we design this for model-based RL. 
- frame.update(delta): update the frame according to the derivative
 we design this for the model-based RL.

- env.observation_space: the description about the frame's observation as the input of the neural network...
- env.derivative_space ...
- env.action_space ...

Define new class is always not a good idea as it will make the concept more complex... the same as before, we just extend the previous idea:
1. allow the environment to add a scene variable.
2. frame_space, 将数据转化成为either dict of tensor/numpy or dict, numpy。sub, metric用来描述两个state之间的差，如何修改，以及metric。
3. observation_space/frame_space.observation_space，描述state_space.observe(state)的结果。
4. derivative_space/frame_space.derivative_space，描述state_space.sub(state1, state2)的结果
5. frame_space(xx, scene) will return a psudo variable, calling the method to frame_space(xxx) will return  


The frame has the following usage:
1. rollout: the rollout function will need to store the state and a sequence of variable/actions.
2. sample two consequent frame from the replay buffer.
3. from the frame get the input to the neural network.. with predefined observation space.
4. calculate the update rule between from one frame to another...
5. metric over two frames...

## TODOs
应该处于也将长期处于复现工作中。

下一周的任务，理清其baseline，然后把手下的task全部接手。
学会submodule。

1. chutong估计搞不定POLO，之后得花一周的时间去做做。
2. jinyu，我应给其提供task支持
3. yingren，我应该给其提供复现支持 （由于这是个测试，所以让他先玩玩
4. yiran，推箱子，和代码支持
5. Zhizuo 是否应该继续

一周之内出mujoco上的实验？
这三天主要做Model方面，GNN的baseline，方便跟jinyu zhao对接。

两周之内应该可以出个基本能看的结果？
Language as an Abstraction for Hierarchical Deep Reinforcement Learning/和Program Guided Agent这个，交给yingren去踩坑。

Yiran，推箱子不知道如何了，似乎可以先用这个试试？

