# Pytorch Implementation of several model-based RL method

Vuong Quan has its a reproducing code which I could use.

So I have the following routines

#Env:
抄一个cartpole过来。

然后用cross entropy method跑通。。

# Model learning in toy examples
理论上我应该可以直接copy quan的代码然后抄抄就好了。

# Model based control?
加个cem?

艹。。这三天就专心吃这坨屎了。

目前计算机学术圈最坑的地方无非就是大家都非常熟悉怎么找个小point发paper这一套理论，于是就不追求能用的东西了。

---

调参方法：
1. 把别人的网络抠过来然后塞到自己的网络里测测。

---

20:59 感觉应该没有太大bug
接下来：
1. 得把training和环境reward测试的统一到一个tensorboard里，想想这个tensorboard什么时候同步的说。
2. 似乎可以加polin了，应该花不了多少时间。
3. 把其它环境给抠出来。。。

---

POPLIN-A-Init:
Neural network policy to propose init

POPLIN-A-Replan:
相当于fix每步的variance的MCTS（vairance就是sample的noise，这个是显然不对的）

POPLIN:
    CEM update policy network
        -UNI: 每步noise一样
        -SEP: 每步noise不一样
    究竟是不是用多元高斯来拟合其实是个问题。
    
Experiments:
    1. Poplin p is the best.
    2. POPLIN-P-Sep is much better than POPLIN-P-Uni
    
---
我需要实现的：
1. collect real data for estimate mean and variance
2. collect imaginary data:
    1. start state?
    2. weight?
    3. returns?
    4. sol?
